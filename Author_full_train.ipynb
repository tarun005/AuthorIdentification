{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Identification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2 : Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split , cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score , confusion_matrix , accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import glob , itertools , add_feature\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset from author folders (Reuters C-50 dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "dfs=[]\n",
    "author_names = sorted(os.listdir('text_corpus/C50train/'))\n",
    "number_authors = len(author_names)\n",
    "for authors in author_names:\n",
    "    article_corpus=[]\n",
    "    for file in glob.glob(os.path.join('text_corpus/C50train',authors,'*.txt')):\n",
    "        with open(file) as fh:\n",
    "            article_corpus.append(fh.read())\n",
    "\n",
    "    dfs.append(pd.DataFrame({'Story':article_corpus , 'Author':authors}))\n",
    "\n",
    "dataset = pd.concat(dfs , axis=0)\n",
    "for author_idx in range(0,50*number_authors,50):\n",
    "    vect = TfidfVectorizer().fit_transform(dataset.iloc[author_idx:author_idx+50,1])\n",
    "    similarity = (vect * vect.T).A\n",
    "    \n",
    "    visited = np.zeros([50])\n",
    "    for i in range(50):\n",
    "        if(not visited[i]):\n",
    "            visited[i] = 1\n",
    "            idx = similarity[i,:] > 0.6\n",
    "            sim_list = np.where(idx)[0]\n",
    "            if len(sim_list) > 0:\n",
    "                for doc_id in sim_list[1:]:\n",
    "                    dataset.iloc[author_idx+doc_id,1] = np.nan\n",
    "                    visited[doc_id] = 1\n",
    "\n",
    "dataset = dataset.dropna()\n",
    "counts = dataset.groupby('Author').agg('count').reset_index()\n",
    "author_shortlist = counts.sort_values('Story',ascending=False).iloc[10:20,0].values\n",
    "dataset_valid = dataset[dataset['Author'].isin(author_shortlist)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change labels to encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = LabelEncoder().fit(dataset_valid['Author'])\n",
    "author_ids = list(encoding.transform(dataset_valid['Author']))\n",
    "dataset_valid['Author_id'] = author_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model and Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dataset_valid['Story'].values\n",
    "y_train = dataset_valid['Author_id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the features using tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(min_df=7,max_df=0.90,ngram_range=(3,5),analyzer='char_wb').fit(X_train)\n",
    "X_train_transform = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model using linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained f1 accuracy:  [ 83.3116026   90.99290231  88.77049335]\n"
     ]
    }
   ],
   "source": [
    "model = SVC(kernel='linear',C=10,class_weight='balanced')\n",
    "\n",
    "score_f1 = cross_val_score(model , X_train_transform , y_train , scoring='f1_weighted')\n",
    "\n",
    "print('Obtained f1 accuracy: ' , score_f1*100)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
